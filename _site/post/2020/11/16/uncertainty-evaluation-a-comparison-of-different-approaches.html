<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Uncertainty evaluation - a comparison of different approaches | yuritan.nl</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="Uncertainty evaluation - a comparison of different approaches">
<meta name="author" content="Yu Ri Tan">
<meta property="og:locale" content="en_US">
<meta name="description" content="After showing two (frequentist) ways of calculating prediction uncerainty, I wanted to see how they compare to each other, and Baysian approach. But how do you evaluate predicted distributions against single true values? And how do you evaluate the uncertainty itself? In this blog, I would like to compare three different methods, trained on a (slightly) more real-life dataset (boston house-prices [3]) by showing a metric used to evaluate single point predictions, and some plots to evaluate the uncertainty. Let’s get right into it.">
<meta property="og:description" content="After showing two (frequentist) ways of calculating prediction uncerainty, I wanted to see how they compare to each other, and Baysian approach. But how do you evaluate predicted distributions against single true values? And how do you evaluate the uncertainty itself? In this blog, I would like to compare three different methods, trained on a (slightly) more real-life dataset (boston house-prices [3]) by showing a metric used to evaluate single point predictions, and some plots to evaluate the uncertainty. Let’s get right into it.">
<link rel="canonical" href="/post/2020/11/16/uncertainty-evaluation-a-comparison-of-different-approaches.html">
<meta property="og:url" content="/post/2020/11/16/uncertainty-evaluation-a-comparison-of-different-approaches.html">
<meta property="og:site_name" content="yuritan.nl">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-11-16T00:00:00+01:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Uncertainty evaluation - a comparison of different approaches">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yu Ri Tan"},"dateModified":"2020-11-16T00:00:00+01:00","datePublished":"2020-11-16T00:00:00+01:00","description":"After showing two (frequentist) ways of calculating prediction uncerainty, I wanted to see how they compare to each other, and Baysian approach. But how do you evaluate predicted distributions against single true values? And how do you evaluate the uncertainty itself? In this blog, I would like to compare three different methods, trained on a (slightly) more real-life dataset (boston house-prices [3]) by showing a metric used to evaluate single point predictions, and some plots to evaluate the uncertainty. Let’s get right into it.","headline":"Uncertainty evaluation - a comparison of different approaches","mainEntityOfPage":{"@type":"WebPage","@id":"/post/2020/11/16/uncertainty-evaluation-a-comparison-of-different-approaches.html"},"url":"/post/2020/11/16/uncertainty-evaluation-a-comparison-of-different-approaches.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="/feed.xml" title="yuritan.nl">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/">yuritan.nl</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Uncertainty evaluation - a comparison of different approaches</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-11-16T00:00:00+01:00" itemprop="datePublished">Nov 16, 2020
      </time></p>
    <img src="/assets/images/2020-11-16-uncertainty-evaluation-a-comparison-of-different-approaches/title_pic.png" alt="Featured image"></header>

  <p></p>
  <div class="post-content e-content" itemprop="articleBody">
    <p>After showing two (frequentist) ways of calculating prediction uncerainty, I wanted to see how they compare to each other, and Baysian approach. But how do you evaluate predicted distributions against single true values? And how do you evaluate the uncertainty itself? In this blog, I would like to compare three different methods, trained on a (slightly) more real-life dataset (boston house-prices [3]) by showing a metric used to evaluate single point predictions, and some plots to evaluate the uncertainty. Let’s get right into it.</p>

<h2 id="first-a-bayesian-approach">First, a Bayesian approach</h2>

<p>There are any different ways to model this, but I’ve chosen to model a bayesian linear regression. I’ve set quite uninformative priors, with alpha ($\alpha$) and beta ($\beta$) coming from a normal distribution with mean ($\mu$) 0 and standard deviation ($\sigma$) 10. With these two distributions, we are able to model $mu$ ($\mu$). Next to this, I would like to model the standard deviation of the target variable that depends on $x$. This resulted into the following model definition:</p>

<h4 id="increasing-variance">Increasing variance:</h4>

<p>$\sigma_{scale} \sim Normal(0,10)$
$\sigma_{bias} \sim HalfNormal(10)$</p>

<p>$\sigma = \sigma{bias} + \sigma{scale} * x$</p>

<h4 id="priors">Priors:</h4>

<p>$\alpha \sim Normal(0, 10)$</p>

<p>$\beta \sim Normal(0, 10)$</p>

<p>Linear Regression: $\mu = \alpha + \beta x$</p>

<h4 id="likelihood">Likelihood:</h4>

<p>$y \sim Normal(\mu, \sigma)$</p>

<p>Defining this was the hardest part, since translating this into PyMC3 code is almost the same.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">sd_scale</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'sd_scale'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sd_bias</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sd_bias'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-5</span>
    <span class="n">sd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sd_bias</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">sd_scale</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'obs'</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>

<p>On <a href="https://github.com/YuRiTan/prediction-uncertainty">Github</a>, you can find a notebook with a more in-depth implementation of this. I’ll show the results in the next part, together with the results of the previous two models. For the next part, I’ve wrapped this model in a Sklearn-like interface with a <code class="language-plaintext highlighter-rouge">.fit()</code> and <code class="language-plaintext highlighter-rouge">.predict()</code>. Since <code class="language-plaintext highlighter-rouge">PyMC3</code> uses a <code class="language-plaintext highlighter-rouge">Theano</code> backend, I had to make use of ‘shared tensors’. Something different compared to tensors from packages like <code class="language-plaintext highlighter-rouge">PyTorch</code>. Without these shared tensors, you were not able to switch train data for test data. The other two models from the previous blog posts got the same SKlearn-like treatment. This allowed me to interact with all three models in the same way, which helps a lot in the next phase. For more details please check the Python code in the <code class="language-plaintext highlighter-rouge">src</code> folder on <a href="https://github.com/YuRiTan/prediction-uncertainty">Github</a>.</p>

<h2 id="visual-comparison">Visual comparison</h2>

<p>Lets start with a visual comparison. I’ll use the toy dataset from the last posts.</p>

<figure>
  <img src="/assets/images/2020-11-16-uncertainty-evaluation-a-comparison-of-different-approaches/three-random-examples-pdf.png" alt="PDF result">
  <figcaption style="text-align: center;"><em>Figure 1: Three random cases from the test set showing the predicted PDF.</em></figcaption>
</figure>

<p>You can see that the three models perform quite similar. This is of course only based on looking at a few samples, but something I think is very nice, considering that there are two frequentist approaches and one Bayesian approach modelling prediction uncertainty. So, the three methods are kind of similar in terms of performance. But how to we evaluate them in more detail? That brings me to the next part: evaluating uncertainty.</p>

<h2 id="evaluation-metrics">Evaluation metrics</h2>

<p>The most simple way of evaluating the model outputs, is taking the average of the predicted distribution and compare it with the ground truth. Here we can use all of our regular evaluation metrics like MSE or MAE. This method however, only says something about the location of the distribution (if symmetrical), and not something about the uncertainty itself. For our toy dataset, the MAE of the mean is as follows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MAE - QRMCDN : 3.843
MAE - HMCDN  : 3.872
MAE - BLR    : 4.012
</code></pre></div></div>

<p>Here it seems that the mean of the two fequentist approaches are a little better than the bayesian approach. So how about the uncertainty? In order to evaluate this, I’ll use the Continuous Ranked Probability Score (CRPS). This metric calculates the area between the predicted distribution as CDF and the actual value as (Heaviside) step function. This sounds quite complicated, but makes more sense when looking at it visually:</p>

<figure>
  <img src="/assets/images/2020-11-16-uncertainty-evaluation-a-comparison-of-different-approaches/TODO" alt="TODO">
  <figcaption style="text-align: center;"><em>Figure 2: Plot showing CRPS for a continuous and empirical CDF.</em></figcaption>
</figure>

<p>So when the location of the distribution is nicely predicted, but the uncertainty is high, the area will be rather large. This also happens when you predict the wrong location (maybe even more so). In order to get a good score (low), you need to predict the location right with a low uncertainty. That is something we would like to do! A nice property is that the metric prevents overly confident distributions. This means that it’s better to be uncertain but rougly right than (un)certain and way off, which makes a lot of sense I think.</p>

<p>Since we often can only draw samples from our predicted distribution, it’s hard to mathmatically calculate the area between the curves. Therefore we use an Empirical CDF[1] instead, which you can see on the right. You can calculate this area by hand of course. But I found a Python package called <code class="language-plaintext highlighter-rouge">properscoring</code>[2] that can calculate this for you. The package isn’t actively maintained anymore, but it still works. In order to compare the three methods equally, we will transform our predicted distributions to (CDF) quantile predictions.</p>

<figure>
  <img src="/assets/images/2020-11-16-uncertainty-evaluation-a-comparison-of-different-approaches/three-random-examples-cdf.png" alt="CDF result">
  <figcaption style="text-align: center;"><em>Figure 3: Three random cases from the test set showing the predicted CDF.</em></figcaption>
</figure>

<p>With these values, we can use <code class="language-plaintext highlighter-rouge">properscoring</code>’s <code class="language-plaintext highlighter-rouge">crps_ensemble()</code> method. This will give you the following scores:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>QRMCDN CRPS: 2.872
HMCDN CRPS : 2.810
BLR CRPS   : 2.925
</code></pre></div></div>

<p>The goal of this is not to show which model is best, since all three models are not fully optimised, but to show how similar they perform.</p>

<h2 id="a-more-real-life-example">A more real life example</h2>

<p>Lets try the Boston housing dataset, which is more realistic than our 2D example. The dataset contains multiple features to eventually predict the housing price. For now, I’ll skip the feature engineering / preprocessing step since the data is quite clean already, and not necessary for the goal of this blog.</p>

<figure>
  <img src="/assets/images/2020-11-16-uncertainty-evaluation-a-comparison-of-different-approaches/boston-pdf.png" alt="Boston housing PDF">
  <figcaption style="text-align: center;"><em>Figure 4: Three random cases showing the predicted PDF and actual value.</em></figcaption>
</figure>

<figure>
  <img src="/assets/images/2020-11-16-uncertainty-evaluation-a-comparison-of-different-approaches/boston-cdf.png" alt="Boston housing CDF">
  <figcaption style="text-align: center;"><em>Figure 5: Three random cases showing the predicted CDF and actual value.</em></figcaption>
</figure>

<p>The uncertainty differs a bit more compared to the toy dataset from before, but is still quite similar. How about the CRPS scores?</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>QRMCDN CRPS: 2.409
HMCDN CRPS : 2.270
BLR CRPS   : 2.409
</code></pre></div></div>

<p>Here we see that the <code class="language-plaintext highlighter-rouge">HMCDN</code>` model predicts best, and the other two equally good. Again, all three models can still be further optimised. Nice to see that our findings still hold after testing on a (slightly) more realistic dataset!</p>

<h3 id="discussion-calibration">Discussion: Calibration</h3>

<p>Now we know how to evaluate a predicted distribution with a single true value, but how well does the distribution represent reality? Does the predicted probability really reflect actual probabilities? One way to check this is by making a calibration plot (or Reliability plot). In order to make this plot, we have to get some quantile values from the predicted distributions. Then you can compare how well this quantile fits the actual quantile. In other words, when predicting the median (quantile .5) we expect that these predictions overpredict in 50% of the cases. The same holds for the other quantiles. If you calculate this for a set of quantiles, you can make the following plot:</p>

<figure>
  <img src="/assets/images/2020-11-16-uncertainty-evaluation-a-comparison-of-different-approaches/TODO" alt="TODO">
  <figcaption style="text-align: center;"><em>Figure 6: Calibration plot for both the toy data set as well as the Boston housing data set.</em></figcaption>
</figure>

<p>The better the curve fits the diagnoal, the better the predicted distributions are calibrated. The distributions predicted on the toy data are quite well calibrated, since the calibration curve is very similar to the diagonal. For the Boston data set it’s a bit more off. You can read this figure as follows. When we predict for quantile .8, we expect to overpredict in 80% of the cases. If we start at the y-axis on the right plot at 0.8, and move to the right until we hit the blue curve. Then go down until the x-axis, we can see that we actually only overpredict in 65-70% of the cases. Our higher quantiles seem to underpredict.</p>

<p>So what to do about this calibration error? We can of course try to optimise the model’s parameters, which will probably help, but can you actually adjust the model itself afterwards? That’s something I would like to find out.</p>

<p>In the last three posts, I’ve discussed two frequentist ways of calculating prediction uncertainty, some uncertainty evaluation methods and a comparison with an actual baysian model. It turned out that you can actually get quite similar results, with having the benefits of using familiar methods (feedforward neural networks) / packages (<code class="language-plaintext highlighter-rouge">PyTorch</code>`) for most data scientists, and scalability for bigger datasets. Let’s hope everyone will incorporate the uncertainty in predictions for regression tasks from now on!</p>

<h2 id="references">References</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/Empirical_distribution_function">https://en.wikipedia.org/wiki/Empirical_distribution_function</a></p>

<p>[2] <a href="https://github.com/TheClimateCorporation/properscoring">https://github.com/TheClimateCorporation/properscoring</a></p>

<p>[3] <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html</a></p>

  </div>
<a class="u-url" href="/post/2020/11/16/uncertainty-evaluation-a-comparison-of-different-approaches.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">yuritan.nl</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Yu Ri Tan</li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list">
<li><a href="https://github.com/YuRiTan"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">YuRiTan</span></a></li>
<li><a href="https://www.linkedin.com/in/yuri-tan"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">yuri-tan</span></a></li>
</ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my blog!</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
